{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 2, Word learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start with a very simple model, of a Bayesian learner learning the meaning of a single word. This model is a simplification of the model in Tenenbaum (2000), which is a simpler version of the model in Xu & Tenenbaum (2007): in other words, it's pretty simple, but allows us to illustrate some of the same principles. It's also our first Bayesian model, so if you haven't seen one of these before then starting with a relatively simple one will help!\n",
    "\n",
    "## Representing word meanings\n",
    "\n",
    "We are going to assume that the meaning of a word is the set of things that word refers to, and to make things even easier for ourselves, we are going to model \"things that words refer to\" as numbers. So a word meaning is just a set of numbers, representing the set of entities that the word can be used to refer to. \n",
    "\n",
    "Python gives us some handy notation for dealing with sets: sets look a bit like lists, but they are enclosed in curly brackets, they are unordered, and they can't contain duplicates. So the following code represents (in Python) the meaning of a word which can be used to refer to three things, entities 1, 2 and 7. \n",
    "\n",
    "```Python\n",
    "word_meaning = {1,2,7}\n",
    "```\n",
    "You'll have to use your imagination to decide what entities 1, 2 and 7 are! Maybe this word is a noun and these are the objects in the world it can refer to, or maybe it's a verb and these are actions it can be used to describe. \n",
    "\n",
    "Using this notation, how would you represent a word with a very specific meaning, referring only to entity 99? How would you represent the meaning of a word that refers to entities 0, 1, 2, 3, and 4? What happens if you try to create a word meaning that contains a duplicate entity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arithmetic combination of numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "Now we've introduced the notation we are going to be using to represent word meanings we can get on and build the model. We'll start by loading the `prod()` function from the numpy library (we will use it for multiplying a list of numbers, it saves us writing our own function). We also have to load the plotting library and set up inline plots. Every model we look at will start with a block of code like this that loads some libraries and sets up plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import prod\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A useful function for normalizing with probabilities\n",
    "\n",
    "We are also going to define a utility function, `normalize_probs()`, which takes a list of numbers and normalizes them for us (i.e. scales them so they sum to 1) – we are going to use this at various points to make sure the numbers we are dealing with are genuine probabilities and behave nicely. This function is used elsewhere in the code, but it is not particularly important that you understand exactly how it works so you can skip over it if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_probs(probs):\n",
    "    \"\"\"Takes a list of numbers; returns a list of scaled versions of those numbers that, together, sum to 1.\"\"\"\n",
    "    total = sum(probs)  # calculates the summed probabilities\n",
    "    normedprobs = []\n",
    "    for p in probs:\n",
    "        normedprobs.append(p / total)  # normalise by dividing by summed probs\n",
    "    return normedprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The hypothesis space\n",
    "As outlined above, word meanings are sets. Our learner is going to receive some data, that is, some examples of the word they are trying to learn being used to refer to entities in the world. Then they will use Bayesian inference to infer the word's meaning, that is, the set of entities the word can be used to refer to.\n",
    "\n",
    "To make this model work, we have to lay out the candidate word meanings our learner considers: this is the *hypothesis space*. Our learner will consider each of the hypotheses (candidate word meanings) in the hypothesis space and calculate its posterior probability given the data they have seen.\n",
    "\n",
    "The hypothesis space is therefore a collection of word meanings that the learner considers when learning. We'll represent the hypothesis space in Python as a list of word meanings. So here's how we represent the hypothesis space for a learner who thinks there are two candidate meanings for the word they are learning: either it refers to entity 0 and entity 1, or it refers to entity 2.\n",
    "\n",
    "```Python\n",
    "toy_hypothesis_space = [{0,1},{2}]\n",
    "```\n",
    "\n",
    "How would you represent the following hypothesis space: \"either the word refers to entity 0, or it refers to entities 2 through 4, or it refers to entity 5\"?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0}, {2, 3, 4}, {5})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{0},{2,3,4},{5}\n",
    "# how does {a,b} vs {a},{b} work in natural language?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could make the set of entities and the list of possible hypotheses as complex as you want, but for this model we are going to start out by making things fairly small (and just big enough to illustrate the things we want to illustrate). \n",
    "- We'll assume there are 11 possible entities in the world, numbered 0 to 10.\n",
    "- We'll assume that the word refers to between 1 and 6 entities (i.e. it doesn't refer to the empty set, it doesn't refer to everything).\n",
    "- We'll assume that words refer to entities that are clustered in the space of possible entities, as represented by consecutive numbers. This is a bit arbitrary and we'll return to this assumption below!\n",
    "\n",
    "Under those assumptions, this is our hypothesis space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hypotheses = [{0},{1},{2},{3},{4},{5},{6},{7},{8},{9},{10},\n",
    "         {0,1},{1,2},{2,3},{3,4},{4,5},{5,6},{6,7},{7,8},{8,9},{9,10},\n",
    "         {0,1,2},{1,2,3},{2,3,4},{3,4,5},{4,5,6},{5,6,7},{6,7,8},{7,8,9},{8,9,10},\n",
    "         {0,1,2,3},{1,2,3,4},{2,3,4,5},{3,4,5,6},{4,5,6,7},{5,6,7,8},{6,7,8,9},{7,8,9,10},\n",
    "         {0,1,2,3,4},{1,2,3,4,5},{2,3,4,5,6},{3,4,5,6,7},{4,5,6,7,8},{5,6,7,8,9},{6,7,8,9,10},\n",
    "         {0,1,2,3,4,5},{1,2,3,4,5,6},{2,3,4,5,6,7},{3,4,5,6,7,8},{4,5,6,7,8,9},{5,6,7,8,9,10}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are just enumerating the hypotheses by hand, rather than writing a function to generate this list for us. Let's have a look at these hypotheses, by typing `all_hypotheses` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0},\n",
       " {1},\n",
       " {2},\n",
       " {3},\n",
       " {4},\n",
       " {5},\n",
       " {6},\n",
       " {7},\n",
       " {8},\n",
       " {9},\n",
       " {10},\n",
       " {0, 1},\n",
       " {1, 2},\n",
       " {2, 3},\n",
       " {3, 4},\n",
       " {4, 5},\n",
       " {5, 6},\n",
       " {6, 7},\n",
       " {7, 8},\n",
       " {8, 9},\n",
       " {9, 10},\n",
       " {0, 1, 2},\n",
       " {1, 2, 3},\n",
       " {2, 3, 4},\n",
       " {3, 4, 5},\n",
       " {4, 5, 6},\n",
       " {5, 6, 7},\n",
       " {6, 7, 8},\n",
       " {7, 8, 9},\n",
       " {8, 9, 10},\n",
       " {0, 1, 2, 3},\n",
       " {1, 2, 3, 4},\n",
       " {2, 3, 4, 5},\n",
       " {3, 4, 5, 6},\n",
       " {4, 5, 6, 7},\n",
       " {5, 6, 7, 8},\n",
       " {6, 7, 8, 9},\n",
       " {7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4},\n",
       " {1, 2, 3, 4, 5},\n",
       " {2, 3, 4, 5, 6},\n",
       " {3, 4, 5, 6, 7},\n",
       " {4, 5, 6, 7, 8},\n",
       " {5, 6, 7, 8, 9},\n",
       " {6, 7, 8, 9, 10},\n",
       " {0, 1, 2, 3, 4, 5},\n",
       " {1, 2, 3, 4, 5, 6},\n",
       " {2, 3, 4, 5, 6, 7},\n",
       " {3, 4, 5, 6, 7, 8},\n",
       " {4, 5, 6, 7, 8, 9},\n",
       " {5, 6, 7, 8, 9, 10}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see the hypotheses that have specific meanings? How many are there? How many candidate word meanings have the broadest, most general meaning? If this is our hypothesis space, would a learner ever consider the possibility that a word had a meaning like \"this word refers to entity 0 or entity 5, but nothing else\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first 11 sets have specific meanings. there are 6 sets with 6 meanings. if a learner considers a word refer to either 0 or 5, the hypothesis space look like {0,5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prior\n",
    "Now we have our space of hypotheses the word learner is going to consider, we need to specify two quantities to do Bayesian inference: \n",
    "- the prior probability of each hypothesis (i.e. before I have seen any data, how likely is it that the word will have this meaning?)\n",
    "- the likelihood of the data given each hypothesis (i.e. if this candidate word meaning was the word's *true* meaning, how likely would it be to generate the data I saw?). \n",
    "\n",
    "We'll do the prior first, since it's very simple: we'll just assume that all word meanings are *a priori* equally likely. In other words, our learner is going to assume, before they have seen any data, that every candidate word meaning in their hypothesis space is an equally good candidate, an equally likely possible word meaning. \n",
    "\n",
    "The way we are doing this in Python is to write a function, `calculate_prior()`, which works through a list of hypotheses and assigns each hypothesis equal weight: it just returns a list of prior probabilities, one per hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(possible_hypotheses): # uniform equal prior\n",
    "    \"\"\"Takes list of sets; returns list of probabilities with one probability per set.\"\"\"\n",
    "    prior = []  # going to put our prior probabilities here\n",
    "    for h in possible_hypotheses:  # for each hypothesis\n",
    "        prior.append(1/len(possible_hypotheses))  #assign it 1/number-of-hypotheses prior probability\n",
    "    return prior  # return the list when finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the prior over our hypothesis space (by running `calculate_prior(all_hypotheses)`). Do the numbers you see there make sense? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549,\n",
       " 0.0196078431372549]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prior(all_hypotheses) # the priors calculated by 1/len are all the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The likelihood\n",
    "Next up, the likelihood: for each candidate word meaning, how likely is it to generate a particular sample of data? In our case we are learning a single word, so our data consists of a sequence of labelling events, where on each labelling event we see the word being used to refer to an entity. We'll represent those sequences of labelling events as a list of numbers, where each element in the list is the entity that the word was referring to on a given labelling event. So for example, if I see the word being used to refer to entity 3 then entity 0, my data looks like this:\n",
    "```Python\n",
    "my_data = [3,0]\n",
    "```\n",
    "And if I see the word being used to label entity 7 five times in a row then entity 9 once, my data will look like this:\n",
    "```Python\n",
    "my_other_data = [7,7,7,7,7,9]\n",
    "```\n",
    "How would the following sequence of exposures be represented? I see the word being used to refer to entity 0, then entity 3, then entity 1, then entity 0 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 1, 0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0,3,1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way of representing data we can calculate the likelihood of a given set of data (a sequence of labellings) given a particular hypothesis about the word's meaning. We'll make two assumptions about labelling:\n",
    "- We'll assume that words can only be used to label entities included in the word meaning (i.e. the word with meaning `{0,1}` can only be used to label entities 0 and 1, never entity 2, 3, etc.)\n",
    "- We'll assume that the **word is equally likely to be used to label any entity included in its meaning** (i.e. the word with meaning `{0, 1}` is equally likely to be used to label entity 0 and entity 1; there is no privileged meaning that it's most likely to be used for). \n",
    "\n",
    "Again, these are assumptions – we could build the model differently, to embody a different set of assumptions encapsulating a different theory of how words work!\n",
    "\n",
    "Under those assumptions, `likelihood()` calculates the likelihood of some data (a sequence of observations of labellings) for a specific hypothesis: it works through a list of data, calculating the probability of each labelling event, then the overall likelihood of the sequence of exposures is just the product of those probabilities (i.e. we multiply all the individual probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(data,hypothesis):\n",
    "    \"\"\"Takes data (list of numbers) and hypothesis (set of numbers);\n",
    "       returns the probability of the given hypothesis generating the observed data.\"\"\"\n",
    "    likelihoods = [] #going to store the probability of each observation here\n",
    "    for data_item in data: #work through the data\n",
    "        if data_item in hypothesis: #if that labelling can be generated under this candidate word meaning\n",
    "            likelihood_this_item = 1/len(hypothesis) #its probability is 1/length-of-the-hypothesis\n",
    "        else: #if that labelling *cannot* be generated under this candidate word meaning\n",
    "            likelihood_this_item = 0 #its probability is 0\n",
    "        likelihoods.append(likelihood_this_item)\n",
    "    return prod(likelihoods) #when you have all the individual likelihoods, multiply them all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the likelihood of observing the word being used to label entity 0 once if the word's meaning is `{0,1}`? Hint: you can calculate this using `likelihood([0],{0,1})`. What's the likelihood of seeing the word being used to label entity 0 twice and entity 1 once, if the word's meaning is `{0,1}`? What's the likelihood of that same data if the word's meaning is `{0,1,2,3}`? Why the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood([0],{0,1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/2, 1/3, 1/4\n",
    "\n",
    "if the data contains any observation not present in the meaning, likelihood will be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood([1,2],{0,1,2})\n",
    "# 1/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The posterior\n",
    "Finally we are in a position to be able to calculate the posterior: our learner receives some data, we work through our hypothesis space (our set of candidate word meanings) and calculate the posterior probability of each hypothesis given that data. We do this by applying Bayes' Rule: the posterior probability of hypothesis $h$ given data $d$ is proportional to the likelihood of $d$ given $h$ times the prior probability of $h$. Or, in maths, $p(h|d)\\propto p(d|h) p(h)$. We have already defined our prior and the likelihood, so all we have to do is work through the hypothesis space and calculate this product for each hypothesis; then, at the end, we normalise these numbers, which turns them into probabilities (this is the equivalent of calculating $p(d)$, which is the denominator of Bayes' Rule). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(data,possible_hypotheses,prior):\n",
    "    \"\"\"Takes data (list of numbers), possible_hypotheses (list of sets), prior (list of probabilities);\n",
    "       returns list of probabilities, one per hypothesis.\"\"\"\n",
    "    posteriors = []  # this will hold the posterior distribution\n",
    "    for i in range(len(possible_hypotheses)):  # consider each possible hypothesis in turn\n",
    "        h = possible_hypotheses[i]  # look up the hypothesis \n",
    "        prior_h = prior[i]  # look up its prior \n",
    "        likelihood_h = likelihood(data,h)  # calculate likelihood of data given this hypothesis\n",
    "        posterior_h = prior_h * likelihood_h  # multiply prior x likelihood\n",
    "        posteriors.append(posterior_h)  # add it to the growing list\n",
    "    return normalize_probs(posteriors)  # finally, normalise to turn these numbers into a probability distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `posterior` function to calculate the posterior probability of all hypotheses (the posterior probability distribution) given the data `[0,0,1]`. You can achieve this using the following little chunk of code.\n",
    "\n",
    "```Python\n",
    "my_prior = calculate_prior(all_hypotheses)\n",
    "posterior([0,0,1],all_hypotheses,my_prior)\n",
    "```\n",
    "If that's a bit hard to look at, you can print out a nice list with each hypothesis and its posterior probability using a `for` loop, like this:\n",
    "```Python\n",
    "my_prior = calculate_prior(all_hypotheses)\n",
    "my_posterior = posterior([0,0,1],all_hypotheses,my_prior)\n",
    "for i in range(len(all_hypotheses)):\n",
    "    print(all_hypotheses[i],my_posterior[i])\n",
    "    \n",
    "```\n",
    "Which hypotheses (candidate word meanings) have posterior probability 0? Why? Which hypotheses have non-zero posterior probability? Which hypothesis has the highest posterior probability, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, {0}\n",
      "0.0, {1}\n",
      "0.0, {2}\n",
      "0.0, {3}\n",
      "0.0, {4}\n",
      "0.0, {5}\n",
      "0.0, {6}\n",
      "0.0, {7}\n",
      "0.0, {8}\n",
      "0.0, {9}\n",
      "0.0, {10}\n",
      "0.6568863586599518, {0, 1}\n",
      "0.0, {1, 2}\n",
      "0.0, {2, 3}\n",
      "0.0, {3, 4}\n",
      "0.0, {4, 5}\n",
      "0.0, {5, 6}\n",
      "0.0, {6, 7}\n",
      "0.0, {8, 7}\n",
      "0.0, {8, 9}\n",
      "0.0, {9, 10}\n",
      "0.19463299515850427, {0, 1, 2}\n",
      "0.0, {1, 2, 3}\n",
      "0.0, {2, 3, 4}\n",
      "0.0, {3, 4, 5}\n",
      "0.0, {4, 5, 6}\n",
      "0.0, {5, 6, 7}\n",
      "0.0, {8, 6, 7}\n",
      "0.0, {8, 9, 7}\n",
      "0.0, {8, 9, 10}\n",
      "0.08211079483249398, {0, 1, 2, 3}\n",
      "0.0, {1, 2, 3, 4}\n",
      "0.0, {2, 3, 4, 5}\n",
      "0.0, {3, 4, 5, 6}\n",
      "0.0, {4, 5, 6, 7}\n",
      "0.0, {8, 5, 6, 7}\n",
      "0.0, {8, 9, 6, 7}\n",
      "0.0, {8, 9, 10, 7}\n",
      "0.042040726954236926, {0, 1, 2, 3, 4}\n",
      "0.0, {1, 2, 3, 4, 5}\n",
      "0.0, {2, 3, 4, 5, 6}\n",
      "0.0, {3, 4, 5, 6, 7}\n",
      "0.0, {4, 5, 6, 7, 8}\n",
      "0.0, {5, 6, 7, 8, 9}\n",
      "0.0, {6, 7, 8, 9, 10}\n",
      "0.024329124394813034, {0, 1, 2, 3, 4, 5}\n",
      "0.0, {1, 2, 3, 4, 5, 6}\n",
      "0.0, {2, 3, 4, 5, 6, 7}\n",
      "0.0, {3, 4, 5, 6, 7, 8}\n",
      "0.0, {4, 5, 6, 7, 8, 9}\n",
      "0.0, {5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "my_prior = calculate_prior(all_hypotheses)\n",
    "posts=posterior([0,0,1],all_hypotheses,my_prior)\n",
    "for i in range (len(all_hypotheses)):\n",
    "    print(f'{posts[i]}, {all_hypotheses[i]}')\n",
    "# hypothesis lacking any observation will have posterior 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "The priority for this worksheet is to work through the in-text questions above: understanding what a hypothesis looks like in this model and how we lay out the hypothesis space, checking that the prior and likelihood functions makes sense to you, and verifying that you can compute a posterior distribution. Once you are happy with that, and if you have time left over, try these questions:\n",
    "1. How does the amount of data influence the posterior distribution? For instance, is the posterior the same after seeing the data `[0,0,1]` and the data `[0,0,1,0,0,1]`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more observations matching the hypothesis would give greater posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.6568863586599518), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.19463299515850427), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.08211079483249398), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.042040726954236926), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.024329124394813034), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "[np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.9018073901245205), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.07917101916045172), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.014090740470695633), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0036938030699500374), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.001237047174382058), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n"
     ]
    }
   ],
   "source": [
    "prior1 = calculate_prior(all_hypotheses)\n",
    "print(posterior([0,0,1],all_hypotheses,prior1))\n",
    "print(posterior([0,0,1,0,0,1],all_hypotheses,prior1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. When are more specific word meanings preferred? When are more general word meanings preferred?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more specific word meanings refer to a smaller set such as {0,1} vs {0,1,2,3}, they are preferred when the data is small and matches all of the elements.\n",
    "more general meanings are preferred to cover longer sequences in case uncovered observation yields 0 probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. This code calculates a probability distribution over possible hypotheses given some data. If you had to commit to a *single* hypothesis, how would you choose one? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose the hypothesis that gives the highest combined posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Do we have any kind of *innateness* in our model? Are there word meanings that our model learner could never learn, no matter what kind of data we gave them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the some meanings don't match the data given, they cannot be learned. if a meaning is not in the hypothesis space, but in the data, the model does not learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [Harder] We are assuming that candidate word meanings are nice and neat: a word refers to a set of entities that are contiguous in some sense (as represented by consecutive integers: i.e. our hypothesis space includes `{3,4,5}` but not `{3,5}` as a candidate meaning). Calculate the posterior probability distribution using the standard hypothesis space and the data `[3,5]`. Which hypothesis has the highest posterior probability? Now add a \"hypothesis with a hole in it\", `{3,5}` to the hypothesis space and recalculate the posterior. Which hypothesis has the highest posterior probability now, and why? Is there a potential problem here if we are trying to model learning? How would we change the model to disfavour this kind of \"hypothesis with a hole in it\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, {0}\n",
      "0.0, {1}\n",
      "0.0, {2}\n",
      "0.0, {3}\n",
      "0.0, {4}\n",
      "0.0, {5}\n",
      "0.0, {6}\n",
      "0.0, {7}\n",
      "0.0, {8}\n",
      "0.0, {9}\n",
      "0.0, {10}\n",
      "0.0, {0, 1}\n",
      "0.0, {1, 2}\n",
      "0.0, {2, 3}\n",
      "0.0, {3, 4}\n",
      "0.0, {4, 5}\n",
      "0.0, {5, 6}\n",
      "0.0, {6, 7}\n",
      "0.0, {8, 7}\n",
      "0.0, {8, 9}\n",
      "0.0, {9, 10}\n",
      "0.0, {0, 1, 2}\n",
      "0.0, {1, 2, 3}\n",
      "0.0, {2, 3, 4}\n",
      "0.23781212841854926, {3, 4, 5}\n",
      "0.0, {4, 5, 6}\n",
      "0.0, {5, 6, 7}\n",
      "0.0, {8, 6, 7}\n",
      "0.0, {8, 9, 7}\n",
      "0.0, {8, 9, 10}\n",
      "0.0, {0, 1, 2, 3}\n",
      "0.0, {1, 2, 3, 4}\n",
      "0.13376932223543397, {2, 3, 4, 5}\n",
      "0.13376932223543397, {3, 4, 5, 6}\n",
      "0.0, {4, 5, 6, 7}\n",
      "0.0, {8, 5, 6, 7}\n",
      "0.0, {8, 9, 6, 7}\n",
      "0.0, {8, 9, 10, 7}\n",
      "0.0, {0, 1, 2, 3, 4}\n",
      "0.08561236623067776, {1, 2, 3, 4, 5}\n",
      "0.08561236623067776, {2, 3, 4, 5, 6}\n",
      "0.08561236623067776, {3, 4, 5, 6, 7}\n",
      "0.0, {4, 5, 6, 7, 8}\n",
      "0.0, {5, 6, 7, 8, 9}\n",
      "0.0, {6, 7, 8, 9, 10}\n",
      "0.059453032104637316, {0, 1, 2, 3, 4, 5}\n",
      "0.059453032104637316, {1, 2, 3, 4, 5, 6}\n",
      "0.059453032104637316, {2, 3, 4, 5, 6, 7}\n",
      "0.059453032104637316, {3, 4, 5, 6, 7, 8}\n",
      "0.0, {4, 5, 6, 7, 8, 9}\n",
      "0.0, {5, 6, 7, 8, 9, 10}\n"
     ]
    }
   ],
   "source": [
    "# standard hypothesis\n",
    "standard_prior = calculate_prior(all_hypotheses)\n",
    "posts=posterior([3,5],all_hypotheses,standard_prior)\n",
    "for i in range (len(all_hypotheses)):\n",
    "    print(f'{posts[i]}, {all_hypotheses[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, {0}\n",
      "0.0, {1}\n",
      "0.0, {2}\n",
      "0.0, {3}\n",
      "0.0, {4}\n",
      "0.0, {5}\n",
      "0.0, {6}\n",
      "0.0, {7}\n",
      "0.0, {8}\n",
      "0.0, {9}\n",
      "0.0, {10}\n",
      "0.0, {0, 1}\n",
      "0.0, {1, 2}\n",
      "0.0, {2, 3}\n",
      "0.0, {3, 4}\n",
      "0.0, {4, 5}\n",
      "0.0, {5, 6}\n",
      "0.0, {6, 7}\n",
      "0.0, {8, 7}\n",
      "0.0, {8, 9}\n",
      "0.0, {9, 10}\n",
      "0.0, {0, 1, 2}\n",
      "0.0, {1, 2, 3}\n",
      "0.0, {2, 3, 4}\n",
      "0.1549186676994578, {3, 4, 5}\n",
      "0.0, {4, 5, 6}\n",
      "0.0, {5, 6, 7}\n",
      "0.0, {8, 6, 7}\n",
      "0.0, {8, 9, 7}\n",
      "0.0, {8, 9, 10}\n",
      "0.0, {0, 1, 2, 3}\n",
      "0.0, {1, 2, 3, 4}\n",
      "0.087141750580945, {2, 3, 4, 5}\n",
      "0.087141750580945, {3, 4, 5, 6}\n",
      "0.0, {4, 5, 6, 7}\n",
      "0.0, {8, 5, 6, 7}\n",
      "0.0, {8, 9, 6, 7}\n",
      "0.0, {8, 9, 10, 7}\n",
      "0.0, {0, 1, 2, 3, 4}\n",
      "0.05577072037180482, {1, 2, 3, 4, 5}\n",
      "0.05577072037180482, {2, 3, 4, 5, 6}\n",
      "0.05577072037180482, {3, 4, 5, 6, 7}\n",
      "0.0, {4, 5, 6, 7, 8}\n",
      "0.0, {5, 6, 7, 8, 9}\n",
      "0.0, {6, 7, 8, 9, 10}\n",
      "0.03872966692486445, {0, 1, 2, 3, 4, 5}\n",
      "0.03872966692486445, {1, 2, 3, 4, 5, 6}\n",
      "0.03872966692486445, {2, 3, 4, 5, 6, 7}\n",
      "0.03872966692486445, {3, 4, 5, 6, 7, 8}\n",
      "0.0, {4, 5, 6, 7, 8, 9}\n",
      "0.0, {5, 6, 7, 8, 9, 10}\n",
      "0.34856700232378, {3, 5}\n"
     ]
    }
   ],
   "source": [
    "# added {3,5}\n",
    "add_hyp=all_hypotheses\n",
    "add_hyp.append({3,5})\n",
    "prior2=calculate_prior(add_hyp)\n",
    "posts2=posterior([3,5],add_hyp,prior2)\n",
    "for i in range (len(add_hyp)):\n",
    "    print(f'{posts2[i]}, {add_hyp[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the modified hypothesis space which includes exactly {3,5} , the original set up only with sequential ones including {3,4,5} has lower probability for the observation [3,5], as \"4\" in the hypothesis is not observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution's answer 5**\n",
    "our learner always favours the most specific hypothesis that is consistent with the data they see\"\n",
    "\"There are two ways to fix this (plus see the answer to Q6). One is to shape the hypothesis space such that generalization is inevitable – that's what I did in the all_hypotheses space, where there are no hypotheses-with-a-hole included. Another way to go is to allow those hypotheses to be included in the space, but to fix the prior to disprefer them (i.e. they have lower prior probability), so the learner will tend to generalize by default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [Harder] We are assuming (in the `likelihood()` function, in the line `likelihood_this_item = 1/len(hypothesis)`) that all meanings of a word are equally likely to be encountered. What other kinds of assumptions might you make? How could you model those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some meanings are more likely than others. make sure they sum to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution's answer 6**\n",
    "each word meaning has a prototype example and then some more fringe cases – so for example, for the word meaning {3,4,5}, maybe most of the time it is used to label entity 4 and only ocasionally to label entities 3 and 5. The way this could be included in the model is to change the likelihood function so that, instead of all entities from a word meaning being equally likely to be sampled, it's the \"middle\" one that's most likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assignment Q1\n",
    "demonstrate suspicious coincidence effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs=[0,0,0,0,1]\n",
    "inc_hyp = [{0,1},{0,1,2},{0,1,2,3},{0,1,2,3,4},{0,1,2,3,4,5},{0,1,2,3,4,5,6}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(1.0)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_priors = calculate_prior(inc_hyp)\n",
    "inc_priors = [0,0.1,0.1,0.1,0.1,0.96]\n",
    "inc_priors = [0,0,0,0,0,1]\n",
    "inc_post= posterior(obs,inc_hyp,inc_priors)\n",
    "inc_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = [f\"H{i+1}\" for i in range(len(inc_hyp))]\n",
    "x = np.arange(len(inc_hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(x - 0.2, inc_priors, width=0.4, label=\"Prior\", alpha=0.7)\n",
    "plt.bar(x + 0.2, inc_post, width=0.4, label=\"Posterior\", alpha=0.7)\n",
    "\n",
    "plt.xticks(x, labels)  # Set x-axis labels\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Prior and Posterior Probabilities of Hypotheses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Tenenbaum, J. B. (2000). Bayesian modeling of human concept learning. In S. Solla, T. Leen, & K. R. Muller (Eds.), *Advances in Neural Information Processing Systems 12* (pp 59–65). Cambridge, MA: MIT Press.\n",
    "\n",
    "Xu, F., & Tenenbaum, J. B. (2007). Word learning as Bayesian inference. *Psychological Review, 114,* 245-272."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
